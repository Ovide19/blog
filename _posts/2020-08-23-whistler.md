---
layout: posts
title: Listening to whistlers using SciPy, pyaudio and PyQt5
published: true
header:
  overlay_image: /assets/images/palmer_overlay.jpg
  overlay_filter: 0.2
tags: [SciPy, pyaudio, PyQt5]
date: 2020-08-23
---

**Work in progress**

Have you ever wondered how Earth's natural magnetic field *sounded* like?
Let's check it out!

In this post, we will write a script allowing to plot the spectrogram of Very Low Frequency (VLF) magnetic data, while simultaneously listening to the associated audio file.
You've read it right: VLF data have their frequency comprised between 300Hz and 300kHz, which overlaps with the frequency range perceived by the human ear, thus making it possible to listen to them.

Let's first grab a sample data file from [the Worldwide Archive of Low-Frequency Data and Observation's website](http://waldo.world/), also known as WALDO. In this example, I will use data collected at the Palmer Antarctica Station. The corresponding time series are stored as one minute chunks: let's go ahead and download the file acquired on January 1, 2012, at 02:10AM, which unzips as a .mat file named PA120101021000_021.mat. 


### 1. Importing libraries

There are quite a few tools needed here, starting with SciPy's `loadmat`, which allows to load the aforementioned .mat file into Python. Since we will turn this file into a .wav audio track, we also need the `wave` module, allowing to work with this format.

```python
import sys
import numpy as np
import wave
from scipy.io.wavfile import write
from scipy.io import loadmat
```

For the display, we use an Anti-Grain Geometry rendering in a Qt5 canvas (`matplotlib.use('Qt5Agg')`). We also import `PyQt5`, a module binding Python with the cross-platform Qt API libraries:

```python
import matplotlib
import matplotlib.pyplot as plt
matplotlib.use('Qt5Agg')
from PyQt5 import QtCore, QtWidgets
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure
```

In order to play the sound file associated with the time series, we also need `pyaudio`, a module providing Python bindings for the PortAudio library:

```python
import pyaudio
```

Last but not least, we want to display the spectrogram and play the audio file *at the same time*, which requires the use of the multiprocessing module:

```python
import multiprocessing
```


### 2. Playing the audio file


Let's first write a function enabling to play the audio file associated with the VLF time series acquired at the Palmer station. 
You can see that the function `audiostream` takes two arguments:
-The first argument is `queue`, an instance of the `multiprocessing.Queue()` class.
-The second argument, `filename` is the name of the .wav file of interest. 

```python
def audiostream(queue, filename):

     #    open stream
    wf = wave.open(filename, 'rb')
    CHUNK=1024
    p = pyaudio.PyAudio()
 
    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),
                     channels=wf.getnchannels(),
                     rate=wf.getframerate(),
                     output=True)
         

    data = wf.readframes(CHUNK)
     
    # play stream
    while len(data) > 0:
         stream.write(data)
         data = wf.readframes(CHUNK)
     
     # stop stream
    stream.stop_stream()
    stream.close()
     
     # close PyAudio
    p.terminate()
```

`audiostream` first opens the .wav file in read mode, as specified by the `'rb'` argument in `wave.open(filename, 'rb')`. Upon instantiation of `PyAudio` (`p = pyaudio.PyAudio()`), it then opens a stream with the desired audio parameters using the `pyaudio.PyAudio.open()` method. The parameters specified here are:
-The number of channels: 1, since we are dealing with a mono file.
-The frame rate, *i.e.* the sampling rate, in our case 100,000 samples per second.
-A flag specifying that there is an output.

The function then writes chunks of data to the stream using `stream.write(data)`, 1024 samples at a time. Once it runs out of samples, it stops the stream, closes it and terminates the PortAudio session.


#### 3. Displaying the data

Now that we have dealt with the audio part of our program, let's work on the display.

```python
class MplCanvas(FigureCanvas):

    def __init__(self, parent=None, width=5, height=2.5, dpi=100):
        fig = Figure(figsize=(width, height), dpi=dpi)
        self.axes = fig.add_subplot(111)
        super(MplCanvas, self).__init__(fig)
```

```python
class MainWindow(QtWidgets.QMainWindow):

    def __init__(self, *args, **kwargs):
        super(MainWindow, self).__init__(*args, **kwargs)

        self.canvas = MplCanvas(self, width=6, height=3.5, dpi=100)
        self.setCentralWidget(self.canvas)
        self.timer_id = 0
        self.ydata = data        
        self.update_plot()
        self.show()
     
        self.timer = QtCore.QTimer()
        self.timer.setInterval(timer_interval_in_ms)
        self.timer.timeout.connect(self.update_plot)
        self.timer.start()

    def update_plot(self):
        if not self.timer_id:
            spectrum, freqs, t, im = self.canvas.axes.specgram(data, Fs=fs, detrend="mean", cmap='hsv')
            self.canvas.axes.set_ylim(0,8000)
            self.canvas.axes.set_xlim(0,duration)
            self.canvas.axes.set_ylabel('Frequency (Hz)')
            self.canvas.axes.set_xlabel('Time (s)')
            self.vline = self.canvas.axes.axvline(x=self.timer_id)
        else:
            self.vline.set_xdata(self.timer_id)
            self.canvas.axes.set_title("Time: "+str("{:.2f}".format(self.timer_id))+"s")
        self.canvas.draw()
        self.timer_id += timer_interval_in_ms / 1000
```

```python
def run_display():
     app = QtWidgets.QApplication(sys.argv)
     w = MainWindow()
     app.exec_()
```

#### 4. Running the main program

```python
if __name__ == '__main__':     
     matfilename = 'PA120101021000_021.mat'
     filename = matfilename.split('.')[0]+'.wav' #Name of the wav file
     fs = 100000 #Actual sampling rate of the geomagnetic data
     x = loadmat(matfilename) 
     data=x['data'][:,0][:] #Array
     write(filename, fs, data)
     timer_interval_in_ms = 470
     
     wf = wave.open(filename, 'rb')
     bytes_per_sample = wf.getsampwidth()
     bits_per_sample  = bytes_per_sample * 8
     dtype = 'int{0}'.format(bits_per_sample)
     channels = wf.getnchannels()
     n_bytes_per_sample=bytes_per_sample
     n_channels=channels
     sampling = fs
     duration=60
     CHUNK = 1024
     audio = np.frombuffer(wf.readframes(int(duration*fs*bytes_per_sample/channels)), dtype=dtype)
     
     
     Q = multiprocessing.Queue()
     
     p1 = multiprocessing.Process(target=run_display)
     p1.start()     
     p2 = multiprocessing.Process(target=audiostream, args=(Q,filename))
     p2.start()
     p1.join()
     p2.join()
```
























