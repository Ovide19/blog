---
layout: posts
title: Listening to whistlers using SciPy, pyaudio and PyQt5
published: true
header:
  overlay_image: /assets/images/palmer_overlay.jpg
  overlay_filter: 0.2
tags: [SciPy, pyaudio, PyQt5]
date: 2020-08-23
---

**WORK IN PROGRESS**

Have you ever wondered how Earth's natural magnetic field *sounded* like?
Let's check it out!

In this post, we will write a script allowing to plot the spectrogram of Very Low Frequency (VLF) magnetic data, while simultaneously listening to the associated audio file.
You've read it right: VLF data have their frequency comprised between 300Hz and 300kHz, which overlaps with the frequency range perceived by the human ear, thus making it possible to listen to them.

Let's first grab a sample data file from [the Worldwide Archive of Low-Frequency Data and Observation's website](http://waldo.world/), also known as WALDO. In this example, I will use data collected at the Palmer Antarctica Station. The corresponding time series are stored as one minute chunks: let's go ahead and download the file acquired on January 1, 2012, at 02:10AM, which unzips as a .mat file named PA120101021000_021.mat. 


### 1. Importing libraries

There are quite a few tools needed here, starting with SciPy's `loadmat`, which allows to load the aforementioned .mat file into Python. We also need the `wave` module to play the audio track.

```python
import sys
import numpy as np
import wave
from scipy.io.wavfile import write
from scipy.io import loadmat
```

For the display, we use an Anti-Grain Geometry rendering in a Qt5 canvas (`matplotlib.use('Qt5Agg')`). We also import `PyQt5`, a module binding Python with the cross-platform Qt API libraries:

```python
import matplotlib
matplotlib.use('Qt5Agg')
from PyQt5 import QtCore, QtWidgets
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure
```

In order to play the sound file associated with the time series, we also need `pyaudio`, a module providing Python bindings for the PortAudio library:

```python
import pyaudio
```

Last but not least, we want to display the spectrogram and play the audio file *at the same time*, which requires the use of the multiprocessing module:

```python
import multiprocessing
```


### 2. Playing the audio file with pyaudio


Let's first write a function, `audiostream`,  enabling to play the audio file associated with the VLF time series acquired at the Palmer station. 
The function takes two arguments:
-The first argument is `queue`, an instance of the `multiprocessing.Queue()` class.
-The second argument, `filename` is the name of the .wav file of interest. 

```python
def audiostream(queue, filename):

     #    open stream
    wf = wave.open(filename, 'rb')
    CHUNK=1024
    p = pyaudio.PyAudio()
 
    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),
                     channels=wf.getnchannels(),
                     rate=wf.getframerate(),
                     output=True)
         

    data = wf.readframes(CHUNK)
     
    # play stream
    while len(data) > 0:
         stream.write(data)
         data = wf.readframes(CHUNK)
     
     # stop stream
    stream.stop_stream()
    stream.close()
     
     # close PyAudio
    p.terminate()
```

`audiostream` first opens the .wav file in read mode, as specified by the `'rb'` argument in `wave.open(filename, 'rb')`. Upon instantiation of `PyAudio` (`p = pyaudio.PyAudio()`), it then opens a stream with the desired audio parameters using the `pyaudio.PyAudio.open()` method. The parameters specified here are:
-The number of channels: 1, since we are dealing with a mono file.
-The frame rate, *i.e.* the sampling rate, in our case 100,000 samples per second.
-A flag specifying that there is an output. Setting the output to `False` will prevent the track from playing.

The function then writes chunks of data to the stream using `stream.write(data)`, 1024 samples at a time. Once it runs out of samples, it stops the stream, closes it and terminates the PortAudio session `p.terminate()`. We're done!


#### 3. Displaying the data

Now that the audio part of the program is out of the way, let's work on the display.
This part is freely based on [this PyQt tutorial](https://www.learnpyqt.com/courses/graphics-plotting/plotting-matplotlib/), explaining how to embed Matplotlib plots within PyQt applications.

Let's first set up a Matplotlib canvas, creating a figure and adding a single set of axes to it, using `fig.add_subplot(111)`: 

```python
class MplCanvas(FigureCanvas):

    def __init__(self, parent=None, width=5, height=2.5, dpi=100):
        fig = Figure(figsize=(width, height), dpi=dpi)
        self.axes = fig.add_subplot(111)
        super(MplCanvas, self).__init__(fig)
```

Let's then define a `MainWindow` class.
This class consists of two methods:
-The `__init__` constructor, initializing the class attributes. This is where we add the newly created MplCanvas widget as the "central widget", using `self.setCentralWidget(self.canvas)`. We also initialize a `timer_id` to 0.
-A method named `update_plot`.

Why update our plot?
Well, we could simply plot the spectrogram of the data as a static image, but in order to better understand how each event actually sounds like, we need some kind of cursor moving along while the soundtrack is playing. For instance, we can animate a vertical bar scrolling along the horizontal time axis. In order to do this, we need to frequently update our plot: this is where the `QTimer` class steps in. Inside of the `__init__` constructor, let's:
-create a timer: `self.timer = QtCore.QTimer()`.
-specify a user-defined interval: `self.timer.setInterval(timer_interval_in_ms)`.
-connect the timeout signal to the `update_plot` slot function: `self.timer.timeout.connect(self.update_plot)`.
-start the timer: `self.timer.start()`.

The only thing left to do is to actually update our plot. Upon its first call, *i.e.* if `timer_id` is zero, `update_plot` creates the spectrogram and plots a vertical bar at x=0. Upon subsequent calls to this function, it simply updates the vertical line, using the current `timer_id`. `timer_id` is then incremented based on the specified user-defined interval.

```python
class MainWindow(QtWidgets.QMainWindow):

    def __init__(self, *args, **kwargs):
        super(MainWindow, self).__init__(*args, **kwargs)

        self.canvas = MplCanvas(self, width=6, height=3.5, dpi=100)
        self.setCentralWidget(self.canvas)
        self.timer_id = 0
        self.ydata = data        
        self.update_plot()
        self.show()
     
        self.timer = QtCore.QTimer()
        self.timer.setInterval(timer_interval_in_ms)
        self.timer.timeout.connect(self.update_plot)
        self.timer.start()

    def update_plot(self):
        if not self.timer_id:
            spectrum, freqs, t, im = self.canvas.axes.specgram(data, Fs=fs, detrend="mean", cmap='hsv')
            self.canvas.axes.set_ylim(0,8000)
            self.canvas.axes.set_xlim(0,duration)
            self.canvas.axes.set_ylabel('Frequency (Hz)')
            self.canvas.axes.set_xlabel('Time (s)')
            self.vline = self.canvas.axes.axvline(x=self.timer_id)
        else:
            self.vline.set_xdata(self.timer_id)
            self.canvas.axes.set_title("Time: "+str("{:.2f}".format(self.timer_id))+"s")
        self.canvas.draw()
        self.timer_id += timer_interval_in_ms / 1000
```

We're almost there! The only thing left to do is to create a `run_display` wrapper function instantiating the `MainWindow` class:


```python
def run_display():
     app = QtWidgets.QApplication(sys.argv)
     w = MainWindow()
     app.exec_()
```

#### 4. Running the main program

Now that the indiviudal components of our program are ready, let's assemble them in the `main` call function.
What the code below does is simply load the .mat file (PA120101021000_021.mat), specify its sampling rate (100,000) and then convert it to a .wav file using Python's `wave` method:  `write(filename, fs, data)`.

The .wav file can then be accessed using `wf = wave.open(filename, 'rb')` in order to determine its characteristics such as the `bits_per_sample`, its `dtype` and its number of channels, so that these parameters can be used by our `audiostream` function.

Now finally comes the time to call our functions! This is done by instantiating a multiprocessing queue (`Q = multiprocessing.Queue()`) and then by calling two processes, `p1` and `p2`. `p1` takes care of the display par of the program, calling our wrapper function `run_display`, while `p2` plays the audio track.


```python
if __name__ == '__main__':     
     matfilename = 'PA120101021000_021.mat'
     filename = matfilename.split('.')[0]+'.wav' #Name of the wav file
     fs = 100000 #Actual sampling rate of the VLF data
     x = loadmat(matfilename) 
     data=x['data'][:,0][:] #Array
     write(filename, fs, data)


     wf = wave.open(filename, 'rb')
     bytes_per_sample = wf.getsampwidth()
     bits_per_sample  = bytes_per_sample * 8
     dtype = 'int{0}'.format(bits_per_sample)
     channels = wf.getnchannels()
     n_bytes_per_sample=bytes_per_sample
     n_channels=channels
     sampling = fs
     timer_interval_in_ms = 470
     duration=60
     CHUNK = 1024
     audio = np.frombuffer(wf.readframes(int(duration*fs*bytes_per_sample/channels)), dtype=dtype)
     
     
     Q = multiprocessing.Queue()
     p1 = multiprocessing.Process(target=run_display)
     p1.start()     
     p2 = multiprocessing.Process(target=audiostream, args=(Q,filename))
     p2.start()
     p1.join()
     p2.join()
```

If all goes well, you should start hearing popping and cracking sounds while your data's spectrogram shows up:

![Spectrogram](/blog/assets/images/whistler.png)

























