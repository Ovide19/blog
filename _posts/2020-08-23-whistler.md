---
layout: posts
title: Listening to whistlers using SciPy, pyaudio and PyQt5
published: true
header:
  overlay_image: /assets/images/palmer_overlay.jpg
  overlay_filter: 0.2
tags: [SciPy, pyaudio, PyQt5]
date: 2020-08-23
---

**WORK IN PROGRESS**

Have you ever wondered how Earth's natural magnetic field *sounded* like?
Let's check it out!

In this post, we will write a script allowing to plot the spectrogram of Very Low Frequency (VLF) magnetic data, while simultaneously listening to the associated audio file.
You've read it right: VLF data have their frequency comprised between 300Hz and 300kHz, which overlaps with the frequency range perceived by the human ear, thus making it possible to listen to them.

Let's first grab a sample data file from [the Worldwide Archive of Low-Frequency Data and Observation's website](http://waldo.world/), also known as WALDO. In this example, I will use data collected at the Palmer Antarctica Station. The corresponding time series are stored as one minute chunks: let's go ahead and download the file acquired on January 1, 2012, at 02:10AM, which unzips as a .mat file named PA120101021000_021.mat. 


### 1. Importing libraries

There are quite a few tools needed here, starting with SciPy's `loadmat`, which allows to load the aforementioned .mat file into Python. In order to convert this file into a .wav audio track, we also need the `wave` module.

```python
import sys
import numpy as np
import wave
from scipy.io.wavfile import write
from scipy.io import loadmat
```

For the display, we use an Anti-Grain Geometry rendering in a Qt5 canvas (`matplotlib.use('Qt5Agg')`). We also import `PyQt5`, a module binding Python with the cross-platform Qt API libraries:

```python
import matplotlib
import matplotlib.pyplot as plt
matplotlib.use('Qt5Agg')
from PyQt5 import QtCore, QtWidgets
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure
```

In order to play the sound file associated with the time series, we also need `pyaudio`, a module providing Python bindings for the PortAudio library:

```python
import pyaudio
```

Last but not least, we want to display the spectrogram and play the audio file *at the same time*, which requires the use of the multiprocessing module:

```python
import multiprocessing
```


### 2. Playing the audio file with pyaudio


Let's first write a function, `audiostream`,  enabling to play the audio file associated with the VLF time series acquired at the Palmer station. 
The function takes two arguments:
-The first argument is `queue`, an instance of the `multiprocessing.Queue()` class.
-The second argument, `filename` is the name of the .wav file of interest. 

```python
def audiostream(queue, filename):

     #    open stream
    wf = wave.open(filename, 'rb')
    CHUNK=1024
    p = pyaudio.PyAudio()
 
    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),
                     channels=wf.getnchannels(),
                     rate=wf.getframerate(),
                     output=True)
         

    data = wf.readframes(CHUNK)
     
    # play stream
    while len(data) > 0:
         stream.write(data)
         data = wf.readframes(CHUNK)
     
     # stop stream
    stream.stop_stream()
    stream.close()
     
     # close PyAudio
    p.terminate()
```

`audiostream` first opens the .wav file in read mode, as specified by the `'rb'` argument in `wave.open(filename, 'rb')`. Upon instantiation of `PyAudio` (`p = pyaudio.PyAudio()`), it then opens a stream with the desired audio parameters using the `pyaudio.PyAudio.open()` method. The parameters specified here are:
-The number of channels: 1, since we are dealing with a mono file.
-The frame rate, *i.e.* the sampling rate, in our case 100,000 samples per second.
-A flag specifying that there is an output. Setting the output to `False` will prevent the track from playing.

The function then writes chunks of data to the stream using `stream.write(data)`, 1024 samples at a time. Once it runs out of samples, it stops the stream, closes it and terminates the PortAudio session `p.terminate()`. We're done!


#### 3. Displaying the data

Now that the audio part of the program is out of the way, let's work on the display.
This part is freely based on [this PyQt tutorial](https://www.learnpyqt.com/courses/graphics-plotting/plotting-matplotlib/), explaining how to embed Matplotlib plots within PyQt applications.

Let's first set up a Matplotlib canvas, creating a figure and adding a single set of axes to it, using `fig.add_subplot(111)`: 

```python
class MplCanvas(FigureCanvas):

    def __init__(self, parent=None, width=5, height=2.5, dpi=100):
        fig = Figure(figsize=(width, height), dpi=dpi)
        self.axes = fig.add_subplot(111)
        super(MplCanvas, self).__init__(fig)
```

Let's then define a `MainWindow` class.
This class consists of two methods:
-The `__init__` constructor, initializing the class attributes. This is where we add the newly created MplCanvas widget as the "central widget", using `self.setCentralWidget(self.canvas)`. We also initialize a `timer_id` to 0.
-A method named `update_plot`.

Why update our plot?
Well, we could simply plot the spectrogram of the data as a static image, but in order to better understand how each event actually sounds like, we need some kind of cursor moving along while the soundtrack is playing. For instance, we can animate a vertical bar scrolling along the horizontal time axis. In order to do this, we need to frequently update our plot: this is where the `QTimer` class steps in. Inside of the `__init__` constructor, let's:
-create a timer: `self.timer = QtCore.QTimer()`.
-specify a user-defined interval: `self.timer.setInterval(timer_interval_in_ms)`.
-connect the timeout signal to the `update_plot` slot function: `self.timer.timeout.connect(self.update_plot)`.
-start the timer: `self.timer.start()`.

The only thing left to do, is to actually update our plot. Upon its first call, *i.e.* if `timer_id` is zero, `update_plot` creates the specgram and plots a vertical bar at x=0. Upon subsequent calls, to this function, it simply updates this vertical line, using the current `timer_id`.

```python
class MainWindow(QtWidgets.QMainWindow):

    def __init__(self, *args, **kwargs):
        super(MainWindow, self).__init__(*args, **kwargs)

        self.canvas = MplCanvas(self, width=6, height=3.5, dpi=100)
        self.setCentralWidget(self.canvas)
        self.timer_id = 0
        self.ydata = data        
        self.update_plot()
        self.show()
     
        self.timer = QtCore.QTimer()
        self.timer.setInterval(timer_interval_in_ms)
        self.timer.timeout.connect(self.update_plot)
        self.timer.start()

    def update_plot(self):
        if not self.timer_id:
            spectrum, freqs, t, im = self.canvas.axes.specgram(data, Fs=fs, detrend="mean", cmap='hsv')
            self.canvas.axes.set_ylim(0,8000)
            self.canvas.axes.set_xlim(0,duration)
            self.canvas.axes.set_ylabel('Frequency (Hz)')
            self.canvas.axes.set_xlabel('Time (s)')
            self.vline = self.canvas.axes.axvline(x=self.timer_id)
        else:
            self.vline.set_xdata(self.timer_id)
            self.canvas.axes.set_title("Time: "+str("{:.2f}".format(self.timer_id))+"s")
        self.canvas.draw()
        self.timer_id += timer_interval_in_ms / 1000
```



```python
def run_display():
     app = QtWidgets.QApplication(sys.argv)
     w = MainWindow()
     app.exec_()
```

#### 4. Running the main program

```python
if __name__ == '__main__':     
     matfilename = 'PA120101021000_021.mat'
     filename = matfilename.split('.')[0]+'.wav' #Name of the wav file
     fs = 100000 #Actual sampling rate of the geomagnetic data
     x = loadmat(matfilename) 
     data=x['data'][:,0][:] #Array
     write(filename, fs, data)
     timer_interval_in_ms = 470
     
     wf = wave.open(filename, 'rb')
     bytes_per_sample = wf.getsampwidth()
     bits_per_sample  = bytes_per_sample * 8
     dtype = 'int{0}'.format(bits_per_sample)
     channels = wf.getnchannels()
     n_bytes_per_sample=bytes_per_sample
     n_channels=channels
     sampling = fs
     duration=60
     CHUNK = 1024
     audio = np.frombuffer(wf.readframes(int(duration*fs*bytes_per_sample/channels)), dtype=dtype)
     
     
     Q = multiprocessing.Queue()
     
     p1 = multiprocessing.Process(target=run_display)
     p1.start()     
     p2 = multiprocessing.Process(target=audiostream, args=(Q,filename))
     p2.start()
     p1.join()
     p2.join()
```
























